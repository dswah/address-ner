{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc2b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"/home/dani/dev/conny-dev/rental-multiclass/data/input/data.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f805ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076ac652",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdataset = datasets.load_dataset(\"json\", data_dir=\"/home/dani/dev/conny-dev/rental-multiclass/data/output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34db8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(cdataset[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25acb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bc1573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "PATTERNS = [\n",
    "    \"\\w+berg\",\n",
    "    \"allee\",\n",
    "    \"str.\",\n",
    "    \"strasse\",\n",
    "    \"straße\",\n",
    "    \"(?<!\\d\\d\\d.)\\d\\d\\d\\d\\d\", # non telephone zip code\n",
    "    \"paul-lincke-ufer\",\n",
    "    \"8c\",\n",
    "    \"LINCKE-UFER\",\n",
    "    \"neukölln\",\n",
    "    \"damm\",\n",
    "    \"germany\",\n",
    "    \"berlin\",\n",
    "    \"münchen\",\n",
    "    \"munchen\",\n",
    "    \"stuttgart\",\n",
    "    \"deutschland\",\n",
    "    \"potsdam\",\n",
    "    \"bremen\",\n",
    "    \"wiesbaden\",\n",
    "    \"hanover\",\n",
    "    \"schwerin\",\n",
    "    \"düsseldorf\",\n",
    "    \"dusseldorf\",\n",
    "    \"mainz\",\n",
    "    \"saarbrücken\",\n",
    "    \"saarbrucken\",\n",
    "    \"dresden\",\n",
    "    \"magdeburg\",\n",
    "    \"kiel\",\n",
    "    \"erfunt\",\n",
    "    \"postfach\", # PO box,\n",
    "    \"adresse\"\n",
    "]\n",
    "\n",
    "PATTERNS = re.compile(r\"|\".join(PATTERNS), flags=re.IGNORECASE)\n",
    "\n",
    "PASS_PATTERNS = [\n",
    "    \"postbank\",\n",
    "    \"amtsgericht\",\n",
    "    \"\\d\\d\\.\\d\\d\\.\\d\\d\\d\\d\"\n",
    "]\n",
    "\n",
    "PASS_PATTERNS = [re.compile(pat, flags=re.IGNORECASE) for pat in PASS_PATTERNS]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d083467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import conny_ml\n",
    "from conny_ml.extraction import entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f638ee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = conny_ml.extraction.entities.RegexParser({\"address_element\": PATTERNS}, group=0)\n",
    "\n",
    "def has_address(text):\n",
    "    return bool(PATTERNS.search(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8080e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from address_ner.address_generator import AddressGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3f1b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = AddressGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268fe8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a76184",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.load_dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69108347",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = datasets.load_dataset(\"../data/output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01393fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[\"train\"].features[\"ner_tags\"].feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc3c10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256d7c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([a[\"properties\"][\"unit\"] for a in generator.addresses if a[\"properties\"][\"unit\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a26ae5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.building_number()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2902b282",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.postcode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b8a72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.city_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac9d6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.city()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713fd960",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.building_number()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5308b8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(generator.addresses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6aa95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "2 ** -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17c806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d131724",
   "metadata": {},
   "outputs": [],
   "source": [
    "(10000000*(2 ** (-np.arange(26, dtype=float)))).astype(int).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905c27f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "random.sample(range(26), counts = (1000000*(2 ** (-np.arange(26, dtype=float)))).astype(int).tolist(), k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8ddd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([a[\"properties\"][\"unit\"] for a in generator.addresses if a[\"properties\"][\"unit\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8b3e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da753c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_address():\n",
    "    return generator.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bd9991",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_kde = pd.read_pickle(\"char_kde.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18758f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "character_distribution = char_kde.resample(1000).flatten()\n",
    "character_distribution[character_distribution<0] = 0\n",
    "character_distribution = character_distribution.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537e7dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9630a5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(text, character_distribution=[10], generator=None):\n",
    "    \"\"\"\n",
    "    process:\n",
    "        - split text by \\n\n",
    "        - remove lines that are \"address-like\"\n",
    "        - collect lines into lists until length is just longer than a sampled character limit\n",
    "        - generate an address for each list \n",
    "        - insert address uniformly at random each list\n",
    "        - randomly join each list of lines with \\n and \\s\n",
    "    \"\"\"\n",
    "    \n",
    "    lines = text.split(\"\\n\")\n",
    "    lines = [line for line in lines if not has_address(line)]\n",
    "\n",
    "    if len(lines) == 0:\n",
    "        return []\n",
    "    \n",
    "    # split into reasonable sum-samples\n",
    "    samples = []\n",
    "    sample = []\n",
    "    length = random.choice(character_distribution)\n",
    "    while lines:\n",
    "        sample.append(lines.pop(0))\n",
    "        \n",
    "        if np.sum([len(line) for line in sample]) >= length:\n",
    "            samples.append(sample)\n",
    "            sample = []\n",
    "            length = random.choice(character_distribution)\n",
    "    \n",
    "    # collect last sample regardless of length\n",
    "    if sample:\n",
    "        samples.append(sample)\n",
    "\n",
    "    # generate addresses and insert uniformly at random\n",
    "    entities = []\n",
    "    for sample in samples:\n",
    "        address = generator()\n",
    "        index = random.randint(0, len(sample))\n",
    "        sample.insert(index, address)\n",
    "        \n",
    "        entity = {}\n",
    "        entity[\"text\"] = address\n",
    "        entity[\"begin\"] = int(np.sum([len(line) for line in sample[:index]]) + len(sample[:index])) # lens of strings + joins\n",
    "        entity[\"end\"] = int(entity[\"begin\"] + len(entity[\"text\"]))\n",
    "        entity[\"label\"] = \"address\"\n",
    "        \n",
    "        entities.append(entity)\n",
    "    \n",
    "    # randomly join sample text with \" \" or \"\\n\"\n",
    "    for i, sample in enumerate(samples):\n",
    "\n",
    "        sample_text = \"\"\n",
    "        while len(sample) > 1:\n",
    "            sample_text += sample.pop(0)\n",
    "            sample_text += random.choice([\" \", \"\\n\"])\n",
    "        sample_text += sample.pop() # add last string\n",
    "        \n",
    "        samples[i] = sample_text\n",
    "        \n",
    "    \n",
    "    # sanity check\n",
    "    for ent, sample in zip(entities, samples):\n",
    "        assert sample[ent[\"begin\"]:ent[\"end\"]] == ent[\"text\"]\n",
    "    \n",
    "    return list(zip(samples, entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13912eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "idx = random.randint(0, N)\n",
    "text = cdataset[\"validation\"][idx][\"text\"]\n",
    "samples = sample(text, generator=generate_address, character_distribution=character_distribution)\n",
    "# spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba6f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(character_distribution, bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cffe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([len(generator.sample()) for i in range(10000)], bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044ec7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array('(50,5)', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a68d31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = cdataset.shuffle()\n",
    "sub = sub.filter(lambda data, idx: idx < 1000, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47370da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d453bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d418c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ents = Parallel(n_jobs=8)(delayed(sample)(\n",
    "    datum[\"text\"],\n",
    "    character_distribution,\n",
    "    generate_address) for datum in sub[\"train\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239485c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ents = list(chain(*train_ents))\n",
    "len(train_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67dbc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence size (capped at 500 chars)\n",
    "plt.hist([len(s[0]) for s in train_ents], bins=np.arange(0,500));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee52cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio of ent text to sentence text\n",
    "plt.hist([len(datum[1][\"text\"]) / len(datum[0]) for datum in train_ents], bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a467a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len of ent text\n",
    "plt.hist([len(datum[1][\"text\"]) for datum in train_ents], bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2f5887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT TO CONLL FORMAT\n",
    "# NEED TO MAP CHARACTER OFFSETS TO TOKENS\n",
    "\n",
    "# USE SPACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999dc082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.training import offsets_to_biluo_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1f2748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.training import offsets_to_biluo_tags, biluo_tags_to_spans\n",
    "from conny_ml.extraction.entities.utils import resolve_overlap, align_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc4f237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "def get_spacy_de_model(model=\"de_core_news_sm\"):\n",
    "\n",
    "    exclude = [\n",
    "        'tok2vec',\n",
    "        'tagger',\n",
    "        'morphologizer',\n",
    "        'parser',\n",
    "        'attribute_ruler',\n",
    "        'lemmatizer',\n",
    "        'ner'\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        nlp = spacy.load(model, exclude=exclude)\n",
    "    except OSError as error:\n",
    "        import subprocess\n",
    "\n",
    "        command = f'poetry run python -m spacy download {model}'\n",
    "        process = subprocess.Popen(command.split(), stdout=subprocess.PIPE)\n",
    "        output, error = process.communicate()\n",
    "\n",
    "    nlp = spacy.load(model, exclude=exclude)\n",
    "    return nlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa87237",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = get_spacy_de_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6586b985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_to_conll(sample, nlp=None, scheme=\"IOB\"):\n",
    "    assert scheme in [\"IOB\", \"BILUO\"]\n",
    "    \n",
    "    nlp = nlp or get_spacy_de_model(model=\"de_core_news_sm\")\n",
    "    doc = nlp(sample[0])\n",
    "    doc_ents = align_entities([sample[1]], doc)\n",
    "    doc_ents = resolve_overlap(doc_ents)\n",
    "    \n",
    "    tags = offsets_to_biluo_tags(doc, [(e[\"begin\"], e[\"end\"], e[\"label\"]) for e in doc_ents])\n",
    "    if scheme == \"IOB\":\n",
    "        for i, tag in enumerate(tags):\n",
    "            if tag[0] == \"U\":\n",
    "                tags[i] = \"B\" + str(tag)[1:]\n",
    "            if tag[0] == \"L\":\n",
    "                tags[i] = \"I\" + str(tag)[1:]\n",
    "                \n",
    "    toks = [str(tok) for tok in doc]\n",
    "    \n",
    "    return {\"tokens\": toks, \"ner_tags\": tags}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3b25c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "idx = random.randint(0, len(train_ents))\n",
    "sample_to_conll(train_ents[idx], nlp=nlp, scheme=\"IOB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec39f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c886905",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.randint(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f98b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_conll(batch, random_state=42, scheme=\"IOB\"):\n",
    "    nlp = get_spacy_de_model(model=\"de_core_news_sm\")\n",
    "    \n",
    "    results = []\n",
    "    for datum in batch:\n",
    "        results.append(sample_to_conll(datum, nlp=nlp, scheme=scheme))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85435cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da11b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "n_workers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d7b899",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = partial(batch_to_conll, random_state=random_state)\n",
    "\n",
    "batches = np.array_split(train_ents, n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d101930",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "OUT = Parallel(n_jobs=n_workers)(delayed(batch_to_conll)(batch) for batch in batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cc68b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT = list(chain(*OUT))\n",
    "len(OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba748181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def flatten_dicts(dicts):\n",
    "    out = defaultdict(list)\n",
    "    keys = list(set(chain(*(tuple(d.keys()) for d in OUT))))\n",
    "    for d in dicts:\n",
    "        for k in keys:\n",
    "            out[k].append(d.get(k))\n",
    "    return dict(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b48d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06192c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset.from_dict(flatten_dicts(OUT))\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8b9824",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(set(chain(*train_data.map(lambda sample: {\"labels\": list(set(sample[\"ner_tags\"]))})[\"labels\"])))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1594aab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.remove(\"O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277fa2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.sort(key=lambda label: label[::-1])\n",
    "labels = [\"O\"] + labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e99dd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7359a5",
   "metadata": {},
   "source": [
    "# HUGGING FACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d775bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86890701",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935ead93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501e3ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path(\"../data/output/o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fdafd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.is_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e2e97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2072b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ad0ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d31d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"].to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10647280",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"].to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a61329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"distilbert-base-multilingual-cased\"\n",
    "# MODEL = \"Davlan/distilbert-base-multilingual-cased-ner-hrl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2d2c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dd8498",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "model_checkpoint = \"distilbert-base-multilingual-cased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd40fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric, ClassLabel, Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268d8287",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.features[\"ner_tags\"] = Sequence(ClassLabel(num_classes=len(labels), names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10f8033",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2id = {k:i for i, k in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f986913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.map(lambda sample: {\"ner_tags\": [tag2id[tag] for tag in sample[\"ner_tags\"]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec7268a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6aaaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ab59de",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = Faker(\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc7e8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Faker.seed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f7f579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2d6ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1acd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05bf7dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67349572",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = train_data.features[f\"{task}_tags\"].feature.names\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a0c77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80f0620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df79b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input = tokenizer(train_data[4][\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062dfa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = tokenized_input.word_ids()\n",
    "aligned_labels = [-100 if i is None else tokenize_and_align_labels(datasets['train'][:5])[4][f\"{task}_tags\"][i] for i in word_ids]\n",
    "print(len(aligned_labels), len(tokenized_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8020ed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd223fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c7e833",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_and_align_labels(train_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e36f9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = train_data.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bde6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e5bf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e587de3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c4699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023bd74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [label_list[i] for i in train_data[4][f\"{task}_tags\"]]\n",
    "metric.compute(predictions=[labels], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9d14fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82321c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d4be53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d993215",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small = tokenized_datasets.shard(100, index=0)\n",
    "train_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c39a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_small = tokenized_datasets.shard(100, index=1)\n",
    "val_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9050cbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_small = tokenized_datasets.shard(100, index=2)\n",
    "test_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da275b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_small,#[\"train\"],\n",
    "    eval_dataset=val_small,#[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21170b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4f8d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fc60fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenized_datasets.shard(1000, 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8640826",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels, _ = trainer.predict(X)\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0761256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06916270",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx += 1\n",
    "X[\"tokens\"][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aedc23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(true_predictions[idx], true_labels[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f762198",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conny_ml",
   "language": "python",
   "name": "conny_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
